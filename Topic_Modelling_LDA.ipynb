{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe6nLfa5Ok3fB41Cw2UqiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smuratsirin/539b-Econometrics-/blob/master/Topic_Modelling_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NK1SfpDiIstG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import random\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv('/content/sample_data/wos_2009.csv', encoding = \"latin1\")  # Ensure the CSV has columns 'title' and 'abstract'\n",
        "\n",
        "# Initialize spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a list of additional stopwords to remove\n",
        "additional_stopwords = {\"ltd\", \"right\", \"c.right\", \"copyright\"}\n",
        "\n",
        "# Preprocess text using spaCy\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    This function preprocesses the input text by:\n",
        "    1. Tokenizing the text: Splitting the text into individual words/tokens.\n",
        "    2. Removing stopwords and punctuation: Filtering out common words, punctuation, and specified additional stopwords that do not contribute to topic identification.\n",
        "    3. Lemmatizing the tokens: Converting words to their base forms (e.g., 'running' to 'run').\n",
        "    \"\"\"\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text.lower() not in additional_stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'abstract' column in the DataFrame\n",
        "df['processed_text'] = df['Abstract'].apply(preprocess)\n",
        "\n",
        "# Vectorization using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "# max_df=0.95: Ignore terms that appear in more than 95% of the documents.\n",
        "# min_df=2: Ignore terms that appear in fewer than 2 documents.\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n",
        "\n",
        "# Convert documents into a format suitable for Gensim's LDA model\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = corpora.Dictionary([text.split() for text in df['processed_text']])\n",
        "# Create a bag-of-words representation of the documents\n",
        "corpus = [dictionary.doc2bow(text.split()) for text in df['processed_text']]\n",
        "\n",
        "# Train the LDA (Latent Dirichlet Allocation) model\n",
        "lda_model = LdaModel(corpus, num_topics=15, id2word=dictionary, passes=15)\n",
        "# num_topics=20: Number of topics to be identified by the model.\n",
        "# passes=15: Number of iterations through the corpus during training.\n",
        "\n",
        "# Assign topics to documents\n",
        "def get_document_topic(bow, model):\n",
        "    \"\"\"\n",
        "    Get the dominant topic for a given document represented as a bag-of-words.\n",
        "    :param bow: Bag-of-words representation of the document.\n",
        "    :param model: Trained LDA model.\n",
        "    :return: Dominant topic for the document.\n",
        "    \"\"\"\n",
        "    topics = model.get_document_topics(bow)\n",
        "    return max(topics, key=lambda x: x[1])[0]  # Return the topic with the highest probability\n",
        "\n",
        "# Apply the function to get the dominant topic for each document\n",
        "df['topic'] = [get_document_topic(bow, lda_model) for bow in corpus]\n",
        "\n",
        "# Extract the most relevant keywords for each topic\n",
        "topics_keywords = lda_model.show_topics(num_topics=20, num_words=15, formatted=False)\n",
        "# num_topics=20: Number of topics to be displayed.\n",
        "# num_words=15: Number of keywords to display for each topic.\n",
        "# formatted=False: Return the topics as lists of words instead of formatted strings.\n",
        "topic_keywords_list = {topic: [word for word, prob in words] for topic, words in topics_keywords}\n"
      ],
      "metadata": {
        "id": "80VDNcJTOJuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated DataFrame to a new CSV file\n",
        "df.to_csv('wos_2009_literature_review_with_topics.csv', index=False)\n",
        "\n",
        "# Prepare the keywords data for export\n",
        "keywords_data = []\n",
        "for topic, keywords in topic_keywords_list.items():\n",
        "    keywords_data.append({'topic': topic, 'keywords': ', '.join(keywords)})\n",
        "\n",
        "# Create a DataFrame for the keywords\n",
        "keywords_df = pd.DataFrame(keywords_data)\n",
        "\n",
        "# Save the keywords DataFrame to a CSV file\n",
        "keywords_df.to_csv('topic_keywords.csv', index=False)\n",
        "\n",
        "# Display the most relevant 15 keywords for each topic\n",
        "for topic, keywords in topic_keywords_list.items():\n",
        "    print(f\"Topic {topic}: {', '.join(keywords)}\")\n",
        "\n",
        "# If you want to return the keywords list in a variable\n",
        "topic_keywords = [{\"topic\": topic, \"keywords\": keywords} for topic, keywords in topic_keywords_list.items()]"
      ],
      "metadata": {
        "id": "Obgezt5sTYot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}